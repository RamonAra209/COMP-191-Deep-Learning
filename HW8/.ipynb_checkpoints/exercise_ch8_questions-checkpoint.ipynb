{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0579-9bcf-4f0e-b222-05c78ef37ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CIFAR 10 dataset for training and validation purposes and apply the following changes on each image:\n",
    "# 1) make it a tensor\n",
    "# 2) normalize it based on the mean and standard deviation among all pixels in each channel (RGB).\n",
    "#Print the size of training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90374899-4072-4ee4-a3c8-a1b0ed19ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make a tertiary classifier that distinguishes between deers, dogs, and horses, labeled as 4, 5, and 7, resp.\n",
    "#Create the subset training and validation datasets for this purpose.\n",
    "#Print the size of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ff49-791b-4e63-9d35-35411a0b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a parameterized CNN with the following details. \n",
    "# The parameter is the number of output channels n after the first convolution.\n",
    "# All kernels are of size 3 by 3.\n",
    "# Convolutions must not change the height and width.\n",
    "# Each convolution is followed by hyperbolic tangent as the activation function, and max pooling of size 2 by 2.\n",
    "# Convolution ayers:\n",
    "# 1) First convolution layer works on the input RGB input. Let's assume there are n kernels in this layer.\n",
    "# 2) Second convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer.\n",
    "# 3) Third convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer. \n",
    "# Fully connected layers:\n",
    "# 1) First fully connected layer works on the result of the preceding max pooling layer. \n",
    "#    This layer is followed by hyperbolic tangent as its activation function.\n",
    "# 2) Second fully connected layer works on the result of the preceding activation function, and emits numbers associated\n",
    "#    with each class.\n",
    "# We will use negative log likelihood to compute the loss. So you may add additional layer(s) to your network.\n",
    "# Note: Since the network is parameterized (n), you'd rather define the CNN as a subclass of nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be13802-cee5-4150-b78c-49fae49d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two networks as instances of the CNN you defined above, with n = 16 and n = 32 respectively. \n",
    "#Print the total number of parameters in each of these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7def331-3852-4cc7-9c97-52a1d5831523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our training functionality is supposed to compute gradient on batches of training data, randlomy selected each time.\n",
    "#To this end, create a training data loader with batch size 32 that randomizes access to each batch.\n",
    "#Also, create a validation data loader with the same batch size that does not randomize access to each batch (no need!)\n",
    "#Print the number of batches in training and validation data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ef16c-813a-459a-a65c-7d710321c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your training function that receives the training loader, model, loss function, optimizer, the device (cpu/gpu), and \n",
    "# number of epochs.\n",
    "#In each epoch, you should go through each training data batch, and:\n",
    "# 1) move data to device\n",
    "# 1) compute the output batch, and accordingly the loss\n",
    "# 2) compute the gradient of loss wrt parameters, and update the parameters\n",
    "#After covering all epochs, your training function must report the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfa4c0-173f-4afd-87ee-e602276f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a separate function that receives the validation data loader as well as the model and computes the validation \n",
    "# accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e6166-15b7-4a07-aab5-eebbb2acc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define device dynamically based on whether CUDA is available or not.\n",
    "#Call the training function on the created training data loader, the created CNN  with n = 16, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f930c-039d-4123-b90d-60fac6d68afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the training function on the created training data loader, the created CNN  with n = 32, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why? \n",
    "# (This can be compared to the fully connected network we created in the last set of exercises.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde7111-bf9d-4a9b-b5df-f782586e60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32. \n",
    "# Is the model overfit? (Yes/No) Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e315a-0f42-4dbf-b641-6c11fbe206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a skip connection in your CNN from the output of second max pooling to the input of 3rd max pooling.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebfaac-1a04-4695-aa15-c83f86290c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider dropout layers after each max pooling in the original CNN, where the probability of zeroing output features is 30%.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33917fa1-51d7-42cb-808d-7caa06ef8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering all the modifications which one works better? Plain CNN, CNN+L2, CNN+Skip, CNN+Dropout?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
